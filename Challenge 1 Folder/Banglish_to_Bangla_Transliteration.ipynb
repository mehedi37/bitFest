{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Banglish to Bangla Transliteration using Transformers\n", "This notebook demonstrates the complete workflow for training a T5 Transformer model to transliterate text from Banglish (Romanized Bengali) to Bangla (Bengali script).\n", "\n", "## Key Steps\n", "1. Load the dataset containing transliteration pairs.\n", "2. Split the dataset into training and validation sets.\n", "3. Preprocess the data using a tokenizer.\n", "4. Initialize a T5 Transformer model for conditional generation.\n", "5. Define training arguments and train the model using `Seq2SeqTrainer`.\n", "\n", "### Prerequisites\n", "- Ensure that the required libraries (transformers, datasets, pandas, torch, sklearn) are installed.\n", "- Download the dataset from Hugging Face's datasets repository.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "from datasets import Dataset\n", "from transformers import T5Tokenizer, T5ForConditionalGeneration\n", "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n", "from sklearn.model_selection import train_test_split\n", "import torch"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Load Dataset\n", "We start by loading the dataset containing Banglish-to-Bangla transliteration pairs. \n", "The dataset is stored in a Parquet file hosted on Hugging Face datasets."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_parquet(\"hf://datasets/SKNahin/bengali-transliteration-data/data/train-00000-of-00001.parquet\")\n", "print(\"Dataset columns:\", df.columns.tolist())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Split Dataset\n", "We split the dataset into training and validation subsets using an 80-20 ratio. \n", "This ensures the model has enough data for learning and validation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n", "train_dataset = Dataset.from_pandas(train_df)\n", "val_dataset = Dataset.from_pandas(val_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Initialize Tokenizer\n", "We use the `T5Tokenizer` for preprocessing the text. \n", "The tokenizer converts Banglish and Bangla text into token IDs suitable for the T5 model."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Preprocessing Function\n", "The `preprocess_function` tokenizes the input Banglish text (`bn`) and target Bangla text (`rm`). \n", "It ensures the data is padded and truncated to the model's expected input length."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def preprocess_function(examples):\n", "    inputs = tokenizer(examples[\"bn\"], padding=\"max_length\", truncation=True)\n", "    targets = tokenizer(examples[\"rm\"], padding=\"max_length\", truncation=True)\n", "    inputs[\"labels\"] = targets[\"input_ids\"]\n", "    inputs[\"decoder_input_ids\"] = targets[\"input_ids\"]\n", "    return inputs\n", "\n", "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n", "tokenized_val = val_dataset.map(preprocess_function, batched=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Initialize Model\n", "The `T5ForConditionalGeneration` model is used for training. \n", "We ensure the model is moved to a GPU if available."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Define Training Arguments\n", "Using the `Seq2SeqTrainingArguments`, we configure the training process with parameters like:\n", "- Learning rate\n", "- Batch size\n", "- Number of epochs\n", "- Evaluation strategy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["training_args = Seq2SeqTrainingArguments(\n", "    output_dir=\"./banglish-to-bangla\",\n", "    run_name=\"banglish-to-bangla-v1\",\n", "    eval_strategy=\"epoch\",\n", "    learning_rate=2e-5,\n", "    per_device_train_batch_size=16,\n", "    per_device_eval_batch_size=16,\n", "    num_train_epochs=3,\n", "    weight_decay=0.01,\n", "    save_total_limit=3,\n", "    label_smoothing_factor=0.1,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Initialize Trainer\n", "The `Seq2SeqTrainer` simplifies the training process, handling training, validation, and evaluation automatically."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["trainer = Seq2SeqTrainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=tokenized_train,\n", "    eval_dataset=tokenized_val,\n", "    tokenizer=tokenizer,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Train the Model\n", "The `train` method trains the model and saves the results to the specified output directory."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Dataset info:\", train_dataset)\n", "trainer.train()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 5}